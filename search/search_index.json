{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kafka Connect POC","text":"<p>The poc is intended to deploy k8s cluster an instance of kafka connect. Actually, the main target of kafka connect is a cosmos mongodb api instance using the official mongo kafka connect at 1.5.1 version.</p> <p></p>"},{"location":"#introduction","title":"Introduction","text":"<p>Kafka Connect is a tool for streaming data between kafka and other systems. It relies entirely on kafka, ensuring high scalability and reliability. The most important concept is that of a connector, which is responsible for managing active streaming tasks. Connectors allow interfacing to different systems such as databases (e.g. postegres, mongodb), queues, caches, full-text search engine (e.g. elastic). There are 2 types of connectors:</p> <ul> <li>source: source connector from which to read data to be ingested into kafka</li> <li>sink: connector to deliver data from kafka to an external system</li> </ul> <p>There are several connectors for mongo:</p> <ul> <li>mongodb connector: officially provided by mongo</li> <li>debezium mongodb connector: made by debezium, an open source product that has a number of different connectors for kafka, transformers and more. Debezium connector for MongoDB</li> </ul> <p>Also, kafka connect exposes REST APIs for connector creation and state monitoring. Here are reported the most commonly used APIs.</p>"},{"location":"#use-cases","title":"Use cases","text":"<p>CDC can be used in a lot of applications:</p> <ul> <li>ETL pipelines</li> <li>Data propagation/replication/migration</li> <li>Event sourcing</li> <li>Reliable microservice message exchange</li> <li>Cache invalidation</li> <li>Update search index (like Elastic)</li> <li>Audit log</li> </ul> <p>Info</p> <p>In this POC we focus on event propagation as a kind of outbox table.</p>"},{"location":"#project-folders","title":"Project folders","text":"<ul> <li><code>kafka-connect-image</code> - Contains a Dockerfile to build a kafka-connect base image with official mongo db connector. It use Gradle to download dependecies from maven, also add opentelemetry java agent to jar in order to use it to instrument the connector.</li> </ul>"},{"location":"deploy-connector/","title":"Deploy connector","text":"<p>Kafka connect expose a lot of REST API allowing to manage the connector lifecycle. You can create a connector by making a POST or PUT http request.</p> <p>Example</p> <p>Create a new connector or update an existing one with name <code>cosmos-connector</code>.</p> <p><code>PUT http://localhost:8083/connectors/cosmos-connector/config</code> with the following payload <pre><code>{\n    \"connector.class\": \"com.mongodb.kafka.connect.MongoSourceConnector\",\n    \"connection.uri\": \"${env:COSMOS_CONNECTION_STRING}\",\n    \"database\": \"&lt;db&gt;\",\n    \"collection\": \"&lt;collection&gt;\",\n    \"topic.namespace.map\": \"{\\\"&lt;db&gt;.&lt;collection&gt;\\\": \\\"&lt;topic_you_want&gt;\\\"}\",\n    \"server.api.version\": \"4.2\",\n    \"copy.existing\": false,\n    \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"key.converter.schemas.enable\": false,\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": false,\n    \"publish.full.document.only\": true,\n    \"output.json.formatter\": \"com.mongodb.kafka.connect.source.json.formatter.SimplifiedJson\",\n    \"heartbeat.interval.ms\": 5000,\n    \"mongo.errors.log.enable\": true,\n    \"output.format.value\": \"json\",\n    \"output.format.key\": \"schema\",\n    \"output.schema.key\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"keySchema\\\",\\\"fields\\\":[{\\\"name\\\":\\\"fullDocument.stream_id\\\",\\\"type\\\":\\\"string\\\"}]}\",\n    \"pipeline\": \"[{\\\"$match\\\":{\\\"operationType\\\":{\\\"$in\\\":[\\\"insert\\\",\\\"update\\\",\\\"replace\\\"]}}},{\\\"$project\\\":{\\\"_id\\\":1,\\\"fullDocument\\\":1,\\\"ns\\\":1,\\\"documentKey\\\":1}}]\"\n}\n</code></pre></p> <p>In the example, the most importat configuration are:</p> <ul> <li><code>connection.uri: ${env:COSMOS_CONNECTION_STRING}</code> - this will resolve the connection string with the environment variable value</li> <li><code>pipeline</code> -  is the pipeline used by mongo to make <code>watch</code> method call. The pipeline must be equals as the one repoted by microsoft documention</li> <li><code>output.schema.key</code> - allows to define the partition key used by kafka producer</li> <li>heartbeat - allows to prevent an invalid token. This require an additional topic to keep heartbeats</li> </ul>"},{"location":"opentelemetry/","title":"Setup OpenTelemetry collector","text":"<p>TBD diagram</p> <p>In order to collect traces from kafka-connect and send it to azure appinsight we must setup an opentelemetry collector. The community offers an easy to use helm chart to configure a collector. The example shows a terraform code to deploy the helm chart and a configuration (<code>values.yml</code>) of collector.</p> Terraform helm chartCollector configuration <pre><code>resource \"helm_release\" \"opentelemetry_collecotr\" {\nchart      = \"opentelemetry-collector\"\nname       = \"opentelemetry-collector\"\nrepository = \"https://open-telemetry.github.io/opentelemetry-helm-charts\"\nversion    = \"0.73.1\"\n\nnamespace = \"...\"\n\nvalues = [\n    templatefile(\"values.yml\", {\n            azure_monitor_connection_string : \"&lt;instrumentation_key&gt;\"\n        })\n    ]\n}\n</code></pre> <pre><code>mode: deployment\nimage:\n  pullPolicy: Always\n\nconfig:\n  receivers:\n    jaeger: null\n    prometheus: null\n    zipkin: null\n    otlp:\n      protocols:\n        grpc:\n          endpoint: 0.0.0.0:4317\n\n  exporters:\n    azuremonitor:\n      instrumentation_key: ${azure_monitor_connection_string}\n\n  service:\n    pipelines:\n      traces:\n        receivers: [otlp]\n        exporters: [debug, azuremonitor]\n      metrics:\n        receivers: [otlp]\n        exporters: [debug, azuremonitor]\n      logs:\n        receivers: [otlp]\n        exporters: [azuremonitor]\n</code></pre> <p>This configuration allows the collector to receive data related to metrics, traces and logs from an OTLP endpoint. The endpoint is exposed through grpc on 4317 port. To enable export to azure we use the <code>contrib</code> implementation of azure exporter which requires appinsight's instrumentation key.</p> <p>In <code>service.pipelines</code> you can define the flow of your data starting from receiver, processor and exporter. For example this snippet instrunct the collector to receive traces from otlp endpoint and exports it to debug (console print) and to azuremonitor. <pre><code>traces:\n    receivers: [otlp]\n    exporters: [debug, azuremonitor]\n</code></pre></p>"},{"location":"opentelemetry/#prometheus-receiver","title":"Prometheus receiver","text":"<p>In order to collect JMX exposed metrics through prometheus we must setup collecto to capture prometheus metrics from pods.</p> <p>TBD</p>"},{"location":"prepare-image/","title":"Prepare docker image","text":""},{"location":"prepare-image/#kafka-connect-image","title":"Kafka connect image","text":"<p><code>kafka-connect-image</code> - Contains a Dockerfile to build a kafka-connect base image with official mongo db connector. It use Gradle to download dependecies from maven, also add opentelemetry java agent to jar in order to use it to instrument the connector.</p> <p><pre><code>cd kafka-connect-image\ndocker build -t &lt;azure_acr&gt;.azurecr.io/kafka-connectors .\n</code></pre> The docker build is multistage:</p> <ol> <li>the first stage downloads connector jars using gradle. Also downloads opentelemetry-javaagent to use it later to instrument connector</li> <li>the final stage use <code>debezium/connect-base</code> which is a base image of kafka connect and copy the connector from previous stage</li> </ol> <p>Tip</p> <ul> <li>To login to azure acr <code>az acr login -n &lt;azure_registry&gt;</code></li> <li>Push to registry <code>docker push cstardcommonacr.azurecr.io/kafka-connectors</code></li> </ul>"},{"location":"prepare-image/#strimzi-image","title":"Strimzi image","text":"<p>TBD</p>"},{"location":"setup/","title":"Setup","text":"<p>The following page will report some terraform code snippets which allows to setup kafka connect instance on k8s.</p>"},{"location":"setup/#setup-eventhub","title":"Setup eventhub","text":"<p>Eventhub has been used as kafka cluster for kafka connect. Each eventhub namespace represent a different kafka cluster where azure apply a limit in terms of number of topics. The standard plan of azure eventhub can support up to 10 topics. A single instance of kakfa connect cluster will use 3 technical topics with compaction policy in order to save and keep data related to jobs config, resume checkpoint and job status. 3 of 10 topics will be used by kafka connect cluster, making available only 7 topics for application purpose.</p> <p>Info</p> <p>Kafka connect automatically create topics on azure eventhub, so you need to provide a connection string with manage permission at   namespace level (not topic level).</p> <pre><code>resource \"azurerm_eventhub_namespace_authorization_rule\" \"kakfa_connect_evh_access_key\" {\n  name                = \"kafka-connect-access-key\"\n  namespace_name      = \"...\"\n  resource_group_name = \"...\"\n  manage              = true\n  listen              = true\n  send                = true\n}\n</code></pre>"},{"location":"setup/#kafka-connect-setup","title":"Kafka Connect setup","text":"<p>The setup of kakfa connect can be done using terraform's kubernetes deployment resource. This deployment example also include liveness and readiness of kafka connect using the rest api exposed by kafka connect. The pod will be marked as ready when starts to return 200 from rest api.</p> DeploymentConfigmapSecrets <pre><code>resource \"kubernetes_deployment\" \"kafka_connect\" {\n  metadata {\n    name = \"kafka-connect\"\n    namespace = \"...\"\n  }\n  spec {\n    replicas = \"1\"\n    selector {\n      match_labels = { app = \"kafka-connect\" }\n    }\n    template {\n      metadata {\n        labels = { app = \"kafka-connect\" }\n      }\n      spec {\n        container {\n          name = \"kafka-connect\"\n          image_pull_policy = \"Always\"\n          image = \"cstardcommonacr.azurecr.io/kafka-connectors\"\n\n          port {\n            container_port = 8083\n          }\n\n          env_from {\n            config_map_ref {\n              name = \"kafka-connect-config\"\n            }\n          }\n\n          env_from {\n            secret_ref {\n              name = \"kafka-connect-secret\"\n            }\n          }\n\n          resources {\n            limits = {\n              cpu = \"400m\"\n              memory = \"512Mi\"\n            }\n          }\n\n          liveness_probe {\n            http_get {\n              path = \"/connectors\"\n              port = \"8083\"\n            }\n\n            initial_delay_seconds = 60\n            period_seconds = 5\n            timeout_seconds = 5\n            success_threshold = 1\n            failure_threshold = 10\n          }\n\n          readiness_probe {\n            http_get {\n              path = \"/connectors\"\n              port = \"8083\"\n            }\n            initial_delay_seconds = 40\n            period_seconds = 10\n            timeout_seconds = 5\n            success_threshold = 1\n            failure_threshold = 3\n          }\n        }\n      }\n    }\n  }\n  depends_on = [\n    kubernetes_config_map.kafka_connect_config,\n    kubernetes_secret.kafka_connect_secret\n  ]\n}\n</code></pre> <pre><code>resource \"kubernetes_config_map\" \"kafka_connect_config\" {\n  metadata {\n    name = \"kafka-connect-config\"\n    namespace = \"...\"\n  }\n\n  data = {\n    GROUP_ID = \"&lt;cluster_id&gt;\" // e.g. rtd-cluster\n    BOOTSTRAP_SERVERS = \"&lt;eventhub_name&gt;.servicebus.windows.net:9093\"\n    CONFIG_STORAGE_TOPIC = \"&lt;name&gt;-configs\" // suggested to use same name\n    OFFSET_STORAGE_TOPIC = \"&lt;name&gt;-offsets\"\n    STATUS_STORAGE_TOPIC = \"&lt;name&gt;-status\"\n    CONNECT_KEY_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\"\n    CONNECT_VALUE_CONVERTER=\"org.apache.kafka.connect.json.JsonConverter\"\n    # worker config\n    CONNECT_SECURITY_PROTOCOL=\"SASL_SSL\"\n    CONNECT_SASL_MECHANISM=\"PLAIN\"\n\n    # producer config\n    CONNECT_PRODUCER_SECURITY_PROTOCOL=\"SASL_SSL\"\n    CONNECT_PRODUCER_SASL_MECHANISM=\"PLAIN\"\n\n    # eventhub configs\n    CONNECT_METADATA_MAX_AGE_MS: \"180000\"\n    CONNECT_CONNECTIONS_MAX_IDLE_MS: \"180000\"\n    CONNECT_MAX_REQUEST_SIZE: \"1000000\"\n\n    # allows to get secrets from environment variables\n    CONNECT_CONFIG_PROVIDERS: \"env\"\n    CONNECT_CONFIG_PROVIDERS_ENV_CLASS: \"org.apache.kafka.common.config.provider.EnvVarConfigProvider\"\n  }\n}\n</code></pre> <pre><code>resource \"kubernetes_secret\" \"kafka_connect_secret\" {\n  metadata {\n    name = \"kafka-connect-secret\"\n    namespace = \"...\"\n  }\n  data = {\n    CONNECT_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"&lt;ConnectionString&gt;\\\";\"\n    CONNECT_PRODUCER_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"&lt;ConnectionString&gt;\\\";\"\n    COSMOS_CONNECTION_STRING: \"&lt;cosmos_connection_string&gt;\"\n  }\n}\n</code></pre>"},{"location":"setup/#configuration","title":"Configuration","text":"<p>Every kafka connect configuration can be provided through environment variabile by using \"CONNECT_\" prefix. Here some of the most important configurations for this POC.</p> <p>Group id allows to setup the kafka cluster name, instances with same group id are part of the same cluster. <pre><code>GROUP_ID=&lt;cluster_name&gt;\n</code></pre></p> <p>This enables to resolve placeholder with environment variable, is useful to hide secret when create a new connector. <pre><code>CONNECT_CONFIG_PROVIDERS: \"env\"\nCONNECT_CONFIG_PROVIDERS_ENV_CLASS: \"org.apache.kafka.common.config.provider.EnvVarConfigProvider\"\n</code></pre></p> <p>Tip</p> <p>If you have an environment variabile called \"SUPERSECRET\" you can inject its value when creating a connector by using ${env:SUPERSECRET} placeholder.</p> <p>Eventhub works with sasl_ssl protocol so you need to setup kafka connect to use the azure's connection string <pre><code>CONNECT_SECURITY_PROTOCOL=\"SASL_SSL\"\nCONNECT_SASL_MECHANISM=\"PLAIN\"\nCONNECT_PRODUCER_SECURITY_PROTOCOL=\"SASL_SSL\"\nCONNECT_PRODUCER_SASL_MECHANISM=\"PLAIN\"\n# as secrets ...\nCONNECT_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"&lt;ConnectionString&gt;\\\";\"\nCONNECT_PRODUCER_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"&lt;ConnectionString&gt;\\\";\"\n</code></pre></p>"},{"location":"setup/#high-availability","title":"High availability","text":"<p>To achieve high availability we must use multiple instance of kakfa connect (aka distributed mode). This can be achieved by setting same group.id across multiple instance. In k8s we can increase the number of pods and kafka will balance jobs of same group.id. K8s cluster runs over multiple virtual machine (nodes), so we need to be tolerant to node fault. This can be achieved by using pod anti-affinity in order to schedule pod execution on different nodes. </p> <p>This snippets allows to schedule pod execution on nodes which doens't already contains the same pod. For example, if you have 3 k8s nodes and 3 replica of kafka-connect pod, you will get each node runs a single pod. Affinity<pre><code>spec {\n    affinity {\n        pod_anti_affinity {\n            required_during_scheduling_ignored_during_execution {\n                topology_key = \"kubernetes.io/hostname\"\n            }\n        }\n    }\n    ...\n}\n</code></pre></p> <p>In this way a fault of a single node will not affect kafka connect cluster which has another pod instance running in another k8s node.</p>"},{"location":"setup/#observability","title":"Observability","text":"<p>Warning</p> <p>Before follow this section, you must setup opentelemetry collector which is necessary to collect data from kafka connect.</p> <pre><code>ENABLE_OTEL = \"true\"\nOTEL_SERVICE_NAME= \"kafka-connect\"\nOTEL_TRACES_EXPORTER=\"otlp\"\nOTEL_METRICS_EXPORTER=\"otlp\"\nOTEL_PROPAGATORS=\"tracecontext\"\nOTEL_EXPORTER_OTLP_ENDPOINT=\"http://opentelemetry-collector.strimzi.svc.cluster.local:4317\"\nOTEL_TRACES_SAMPLER=\"always_on\"\nJAVA_TOOL_OPTIONS=\"-javaagent:./opentelemetry-javaagent.jar\"\nOTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=\"false\"\nOTEL_INSTRUMENTATION_MONGO_ENABLED=\"true\"\nOTEL_INSTRUMENTATION_KAFKA_ENABLED=\"true\"\n</code></pre>"},{"location":"setup/#strimzi-setup","title":"Strimzi setup","text":"<p>TBD</p>"}]}